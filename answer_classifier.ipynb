{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8240097,"sourceType":"datasetVersion","datasetId":4887982}],"dockerImageVersionId":30302,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, get_cosine_schedule_with_warmup, AdamW","metadata":{"execution":{"iopub.status.busy":"2024-04-26T22:32:19.211471Z","iopub.execute_input":"2024-04-26T22:32:19.211791Z","iopub.status.idle":"2024-04-26T22:32:22.345272Z","shell.execute_reply.started":"2024-04-26T22:32:19.211724Z","shell.execute_reply":"2024-04-26T22:32:22.344316Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/vtoroy-pilot/train.csv')\n\nCLASSES = list(train['answer_class'].unique())\nCLASSES","metadata":{"execution":{"iopub.status.busy":"2024-04-26T22:32:29.476647Z","iopub.execute_input":"2024-04-26T22:32:29.477305Z","iopub.status.idle":"2024-04-26T22:32:29.501307Z","shell.execute_reply.started":"2024-04-26T22:32:29.477268Z","shell.execute_reply":"2024-04-26T22:32:29.500410Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"[11,\n 15,\n 27,\n 21,\n 25,\n 10,\n 2,\n 6,\n 22,\n 3,\n 12,\n 24,\n 26,\n 0,\n 5,\n 17,\n 1,\n 4,\n 14,\n 13,\n 23,\n 8,\n 20,\n 7,\n 18,\n 9,\n 29,\n 16,\n 28,\n 19]"},"metadata":{}}]},{"cell_type":"markdown","source":"### Manual cleaning of text from job offers and advertisements\n","metadata":{"execution":{"iopub.status.busy":"2022-12-19T13:48:49.380865Z","iopub.execute_input":"2022-12-19T13:48:49.381298Z","iopub.status.idle":"2022-12-19T13:48:49.387744Z","shell.execute_reply.started":"2022-12-19T13:48:49.381256Z","shell.execute_reply":"2022-12-19T13:48:49.386745Z"}}},{"cell_type":"code","source":"labels = dict(zip(CLASSES, range(len(CLASSES))))\n\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, df, tokenizer, phase='train'):\n        self.phase = phase\n        \n        if self.phase == 'train':\n            self.labels = [labels[label] for label in df['answer_class']]\n        elif self.phase == 'test':\n            self.oid = [oid for oid in df['oid']]\n            \n        self.texts = [tokenizer(text, \n                               padding='max_length', max_length = 512, truncation=True,\n                                return_tensors=\"pt\") for text in df['text']]\n\n    def classes(self):\n        return self.labels\n\n    def __len__(self):\n        if self.phase == 'train':\n            return len(self.labels)\n        elif self.phase == 'test':\n            return len(self.oid)\n\n    def get_batch_labels(self, idx):\n        return np.array(self.labels[idx])\n    \n    def get_batch_oid(self, idx):\n        return np.array(self.oid[idx])\n\n    def get_batch_texts(self, idx):\n        return self.texts[idx]\n\n    def __getitem__(self, idx):\n        if self.phase == 'train':\n            batch_texts = self.get_batch_texts(idx)\n            batch_y = self.get_batch_labels(idx)\n            return batch_texts, batch_y\n        elif self.phase == 'test':\n            batch_texts = self.get_batch_texts(idx)\n            batch_oid = self.get_batch_oid(idx)\n            return batch_texts, batch_oid\n   ","metadata":{"execution":{"iopub.status.busy":"2024-04-26T22:32:32.432089Z","iopub.execute_input":"2024-04-26T22:32:32.432521Z","iopub.status.idle":"2024-04-26T22:32:32.447260Z","shell.execute_reply.started":"2024-04-26T22:32:32.432483Z","shell.execute_reply":"2024-04-26T22:32:32.446224Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class BertClassifier:\n    def __init__(self, model_path, tokenizer_path, data, n_classes=13, epochs=5):\n        self.model = BertForSequenceClassification.from_pretrained(model_path)\n        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n        self.data = data\n        self.device = torch.device('cuda')\n        self.max_len = 512\n        self.epochs = epochs\n        self.out_features = self.model.bert.encoder.layer[1].output.dense.out_features\n        self.model.classifier = torch.nn.Linear(self.out_features, n_classes).to(self.device)\n        self.model = self.model.to(self.device)\n\n    \n    def preparation(self):\n        self.df_train, self.df_val = np.split(self.data.sample(frac=1, random_state=42), \n                                     [int(.85*len(self.data))])\n        \n        self.train, self.val = CustomDataset(self.df_train, self.tokenizer, phase='train'), CustomDataset(self.df_val, self.tokenizer, phase='train')\n        self.train_dataloader = torch.utils.data.DataLoader(self.train, batch_size=4, shuffle=True)\n        self.val_dataloader = torch.utils.data.DataLoader(self.val, batch_size=4)\n    \n       \n        self.optimizer = AdamW(self.model.parameters(), lr=2e-5, correct_bias=False)\n        self.scheduler = get_cosine_schedule_with_warmup(\n                self.optimizer,\n                num_warmup_steps=0,\n                num_training_steps=len(self.train_dataloader) * self.epochs\n            )\n        self.loss_fn = torch.nn.CrossEntropyLoss().to(self.device)\n            \n    def fit(self):\n        self.model = self.model.train()\n        \n        for epoch_num in range(self.epochs):\n            total_acc_train = 0\n            total_loss_train = 0\n            for train_input, train_label in tqdm(self.train_dataloader):\n                train_label = train_label.to(self.device)\n                mask = train_input['attention_mask'].to(self.device)\n                input_id = train_input['input_ids'].squeeze(1).to(self.device)\n                output = self.model(input_id.to(self.device), mask.to(self.device))\n\n                batch_loss = self.loss_fn(output[0], train_label.long())\n                total_loss_train += batch_loss.item()\n\n                acc = (output[0].argmax(dim=1) == train_label).sum().item()\n                total_acc_train += acc\n\n                self.model.zero_grad()\n                batch_loss.backward()\n                self.optimizer.step()\n                self.scheduler.step()\n            total_acc_val, total_loss_val, f1 = self.eval()\n           \n            print(\n            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(self.df_train): .3f} \\\n            | Train Accuracy: {total_acc_train / len(self.df_train): .3f} \\\n            | Val Loss: {total_loss_val / len(self.df_val): .3f} \\\n            | Val Accuracy: {total_acc_val / len(self.df_val): .3f} \\\n            | Val F1: {f1: .3f}')\n\n            \n            os.makedirs('checkpoint', exist_ok=True)\n            torch.save(self.model, f'checkpoint/BertClassifier{epoch_num}.pt')\n\n        return total_acc_train, total_loss_train\n    \n    def eval(self):\n        self.model = self.model.eval()\n        total_acc_val = 0\n        total_loss_val = 0\n        y_true = []\n        y_pred = []\n\n        with torch.no_grad():\n            for val_input, val_label in tqdm(self.val_dataloader):\n                val_label = val_label.to(self.device)\n                mask = val_input['attention_mask'].to(self.device)\n                input_id = val_input['input_ids'].squeeze(1).to(self.device)\n\n                output = self.model(input_id.to(self.device), mask.to(self.device))\n\n                batch_loss = self.loss_fn(output[0], val_label.long())\n                total_loss_val += batch_loss.item()\n\n                acc = (output[0].argmax(dim=1) == val_label).sum().item()\n                pred_label = output[0].argmax(dim=1)\n                y_pred.extend(pred_label)\n                y_true.extend(val_label)\n                total_acc_val += acc\n        y_true_tensor = torch.tensor(y_true)\n        y_pred_tensor = torch.tensor(y_pred)\n        f1 = f1_score(y_true_tensor, y_pred_tensor, average='micro')\n        \n        return total_acc_val, total_loss_val, f1\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-26T22:34:00.838367Z","iopub.execute_input":"2024-04-26T22:34:00.838854Z","iopub.status.idle":"2024-04-26T22:34:00.867251Z","shell.execute_reply.started":"2024-04-26T22:34:00.838819Z","shell.execute_reply":"2024-04-26T22:34:00.866208Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model_path = 'cointegrated/rubert-tiny2'\ntokenizer_path = 'cointegrated/rubert-tiny2'\nbert_tiny = BertClassifier(model_path, tokenizer_path, train,n_classes = 30, epochs=15)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T22:36:30.461464Z","iopub.execute_input":"2024-04-26T22:36:30.462304Z","iopub.status.idle":"2024-04-26T22:36:31.699177Z","shell.execute_reply.started":"2024-04-26T22:36:30.462263Z","shell.execute_reply":"2024-04-26T22:36:31.698345Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nbert_tiny.preparation()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T22:36:35.872674Z","iopub.execute_input":"2024-04-26T22:36:35.873500Z","iopub.status.idle":"2024-04-26T22:36:36.366344Z","shell.execute_reply.started":"2024-04-26T22:36:35.873461Z","shell.execute_reply":"2024-04-26T22:36:36.365235Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"CPU times: user 483 ms, sys: 18.7 ms, total: 502 ms\nWall time: 488 ms\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"bert_tiny.fit()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T22:36:38.660407Z","iopub.execute_input":"2024-04-26T22:36:38.661293Z","iopub.status.idle":"2024-04-26T22:37:30.995438Z","shell.execute_reply.started":"2024-04-26T22:36:38.661259Z","shell.execute_reply":"2024-04-26T22:37:30.994507Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"100%|██████████| 118/118 [00:03<00:00, 37.62it/s]\n100%|██████████| 21/21 [00:00<00:00, 138.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 1 | Train Loss:  0.718             | Train Accuracy:  0.465             | Val Loss:  0.544             | Val Accuracy:  0.783             | Val F1:  0.783\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 118/118 [00:02<00:00, 40.04it/s]\n100%|██████████| 21/21 [00:00<00:00, 138.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 2 | Train Loss:  0.415             | Train Accuracy:  0.883             | Val Loss:  0.376             | Val Accuracy:  0.819             | Val F1:  0.819\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 118/118 [00:02<00:00, 40.22it/s]\n100%|██████████| 21/21 [00:00<00:00, 138.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 3 | Train Loss:  0.272             | Train Accuracy:  0.947             | Val Loss:  0.280             | Val Accuracy:  0.867             | Val F1:  0.867\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 118/118 [00:02<00:00, 40.19it/s]\n100%|██████████| 21/21 [00:00<00:00, 136.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 4 | Train Loss:  0.193             | Train Accuracy:  0.979             | Val Loss:  0.226             | Val Accuracy:  0.880             | Val F1:  0.880\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 118/118 [00:02<00:00, 40.14it/s]\n100%|██████████| 21/21 [00:00<00:00, 137.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 5 | Train Loss:  0.146             | Train Accuracy:  0.983             | Val Loss:  0.192             | Val Accuracy:  0.904             | Val F1:  0.904\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 118/118 [00:02<00:00, 40.28it/s]\n100%|██████████| 21/21 [00:00<00:00, 137.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 6 | Train Loss:  0.118             | Train Accuracy:  0.987             | Val Loss:  0.170             | Val Accuracy:  0.904             | Val F1:  0.904\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 118/118 [00:02<00:00, 40.21it/s]\n100%|██████████| 21/21 [00:00<00:00, 138.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 7 | Train Loss:  0.097             | Train Accuracy:  0.991             | Val Loss:  0.155             | Val Accuracy:  0.916             | Val F1:  0.916\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 118/118 [00:02<00:00, 40.21it/s]\n100%|██████████| 21/21 [00:00<00:00, 137.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 8 | Train Loss:  0.083             | Train Accuracy:  0.994             | Val Loss:  0.142             | Val Accuracy:  0.928             | Val F1:  0.928\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 118/118 [00:02<00:00, 40.27it/s]\n100%|██████████| 21/21 [00:00<00:00, 137.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 9 | Train Loss:  0.075             | Train Accuracy:  0.996             | Val Loss:  0.137             | Val Accuracy:  0.928             | Val F1:  0.928\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 118/118 [00:02<00:00, 40.18it/s]\n100%|██████████| 21/21 [00:00<00:00, 136.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 10 | Train Loss:  0.069             | Train Accuracy:  0.996             | Val Loss:  0.131             | Val Accuracy:  0.940             | Val F1:  0.940\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 118/118 [00:02<00:00, 40.17it/s]\n100%|██████████| 21/21 [00:00<00:00, 136.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 11 | Train Loss:  0.065             | Train Accuracy:  0.994             | Val Loss:  0.128             | Val Accuracy:  0.940             | Val F1:  0.940\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 118/118 [00:02<00:00, 40.29it/s]\n100%|██████████| 21/21 [00:00<00:00, 137.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 12 | Train Loss:  0.062             | Train Accuracy:  0.994             | Val Loss:  0.126             | Val Accuracy:  0.940             | Val F1:  0.940\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 118/118 [00:02<00:00, 40.22it/s]\n100%|██████████| 21/21 [00:00<00:00, 136.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 13 | Train Loss:  0.061             | Train Accuracy:  0.996             | Val Loss:  0.125             | Val Accuracy:  0.940             | Val F1:  0.940\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 118/118 [00:02<00:00, 40.21it/s]\n100%|██████████| 21/21 [00:00<00:00, 138.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 14 | Train Loss:  0.060             | Train Accuracy:  0.996             | Val Loss:  0.125             | Val Accuracy:  0.940             | Val F1:  0.940\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 118/118 [00:02<00:00, 40.20it/s]\n100%|██████████| 21/21 [00:00<00:00, 138.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 15 | Train Loss:  0.060             | Train Accuracy:  0.996             | Val Loss:  0.125             | Val Accuracy:  0.940             | Val F1:  0.940\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(467, 27.967378929257393)"},"metadata":{}}]},{"cell_type":"code","source":"test_dataset = CustomDataset(test, bert_tiny.tokenizer, phase='test')\ntest_dataloader = DataLoader(test_dataset, batch_size=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(model, dataloader):\n    all_oid = []\n    all_labels = []\n    label_prob = []\n    \n    model.cuda()\n    model.eval()\n    with torch.no_grad():\n        for test_input, test_oid in tqdm(dataloader):\n            test_oid = test_oid.cuda()\n            mask = test_input['attention_mask'].cuda()\n            input_id = test_input['input_ids'].squeeze(1).cuda()\n            output = model(input_id, mask)\n            all_oid.extend(test_oid)\n            all_labels.extend(torch.argmax(output[0].softmax(1), dim=1))\n            \n            for prob in output[0].softmax(1):\n                label_prob.append(prob)\n        return ([oid.item() for oid in all_oid], [CLASSES[labels] for labels in all_labels], label_prob)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inference_model = torch.load(f'/kaggle/working/checkpoint/BertClassifier{bert_tiny.epochs-1}.pt')\ninference_result = inference(inference_model, test_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oid = [i for i in inference_result[0]]\nlabels = [i for i in inference_result[1]]\nprob = [i for i in inference_result[2]]\nprint(len(dict(zip(oid, labels))))\nprint(len(set(oid) & set(test['oid'].unique())))\nprint(len(set(oid) & set(test['oid'].unique())))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detached_prob = []\nfor i in prob:\n    detached_prob.append(i.cpu().numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = {'oid':oid, 'category':labels, 'probs':detached_prob}\nsubmit = pd.DataFrame(data)\nsubmit['label_int'] = submit['category'].apply(lambda x: CLASSES.index(x))\n\nlabel_int = submit['label_int'].to_list()\nprobs = submit['probs'].to_list()\nres = []\nfor indx, tensor in enumerate(probs):\n\n    res.append(tensor[label_int[indx]])\nsubmit['prob'] = res\ndel submit['probs'], submit['label_int']\ntmp_submit = pd.DataFrame(submit.groupby(by=['oid', 'category']).sum().reset_index())\n\noid = tmp_submit['oid'].to_list()\ncategory = tmp_submit['category'].to_list()\nprob = tmp_submit['prob'].to_list()\n\nres = {}\nfor indx, id in enumerate(oid):\n    if id not in res:\n        res[id] = (category[indx], prob[indx])\n        \nsubmit_data = {k:v[0] for k,v in res.items()}\noid = list(submit_data.keys())\ncategory = list(submit_data.values())\npd.DataFrame({'oid':oid, 'category':category}).to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}